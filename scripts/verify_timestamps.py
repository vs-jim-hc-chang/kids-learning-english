#!/usr/bin/env python3
"""
é©—è­‰ sentences.ts ä¸­çš„æ™‚é–“æˆ³æ˜¯å¦èˆ‡ YouTube å­—å¹•åŒ¹é…
ä½¿ç”¨æ–¹å¼: pipx run --spec youtube-transcript-api python3 scripts/verify_timestamps.py
"""

import re
import json
import time
from pathlib import Path
from difflib import SequenceMatcher

try:
    from youtube_transcript_api import YouTubeTranscriptApi
except ImportError:
    print("è«‹ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤åŸ·è¡Œæ­¤è…³æœ¬ï¼š")
    print("pipx run --spec youtube-transcript-api python3 scripts/verify_timestamps.py")
    exit(1)


def parse_sentences_ts(file_path: str) -> list[dict]:
    """è§£æ sentences.ts æª”æ¡ˆï¼Œæå–å¥å­è³‡æ–™"""
    content = Path(file_path).read_text(encoding='utf-8')

    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–æ¯å€‹å¥å­ç‰©ä»¶
    pattern = r'\{\s*id:\s*(\d+),\s*english:\s*"([^"]+)",\s*chinese:\s*"([^"]+)",\s*videoId:\s*"([^"]+)",\s*startTime:\s*([\d.]+),\s*endTime:\s*([\d.]+)'

    sentences = []
    for match in re.finditer(pattern, content):
        sentences.append({
            'id': int(match.group(1)),
            'english': match.group(2),
            'chinese': match.group(3),
            'videoId': match.group(4),
            'startTime': float(match.group(5)),
            'endTime': float(match.group(6))
        })

    return sentences


def get_cache_path(video_id: str) -> Path:
    """å–å¾—å­—å¹•å¿«å–è·¯å¾‘"""
    cache_dir = Path(__file__).parent / '.cache'
    cache_dir.mkdir(exist_ok=True)
    return cache_dir / f'{video_id}.json'


def fetch_transcript(video_id: str, use_cache: bool = True) -> list[dict] | None:
    """å–å¾— YouTube å½±ç‰‡å­—å¹•ï¼ˆæ”¯æ´å¿«å–ï¼‰"""
    cache_path = get_cache_path(video_id)

    # å˜—è©¦å¾å¿«å–è®€å–
    if use_cache and cache_path.exists():
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                print(f"   ğŸ“¦ ä½¿ç”¨å¿«å–")
                return json.load(f)
        except Exception:
            pass

    # å¾ YouTube å–å¾—
    try:
        ytt_api = YouTubeTranscriptApi()
        transcript = ytt_api.fetch(video_id)
        result = [{'start': item.start, 'text': item.text} for item in list(transcript)]

        # å„²å­˜åˆ°å¿«å–
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"   ğŸ’¾ å·²å¿«å–å­—å¹•")

        return result
    except Exception as e:
        print(f"  âŒ ç„¡æ³•å–å¾—å­—å¹•: {e}")
        return None


def normalize_text(text: str) -> str:
    """æ­£è¦åŒ–æ–‡å­—ä»¥ä¾¿æ¯”è¼ƒ"""
    # ç§»é™¤æ¨™é»ç¬¦è™Ÿå’Œå¤šé¤˜ç©ºæ ¼ï¼Œè½‰å°å¯«
    text = re.sub(r'[^\w\s]', '', text.lower())
    return ' '.join(text.split())


def find_best_match(sentence: str, transcript: list[dict], expected_start: float) -> dict | None:
    """åœ¨å­—å¹•ä¸­æ‰¾åˆ°æœ€åŒ¹é…çš„ç‰‡æ®µï¼Œå„ªå…ˆé¸æ“‡é è¿‘é æœŸæ™‚é–“çš„"""
    sentence_normalized = normalize_text(sentence)
    sentence_words = set(sentence_normalized.split())
    word_count = len(sentence_words)

    # å°æ–¼çŸ­å¥ï¼ˆå°‘æ–¼ 4 å€‹å­—ï¼‰ï¼Œæ™‚é–“æ¥è¿‘åº¦æ¬Šé‡æ›´é«˜
    is_short_sentence = word_count < 4

    candidates = []

    for i, item in enumerate(transcript):
        # çµ„åˆç•¶å‰å’Œæ¥ä¸‹ä¾†å¹¾å€‹å­—å¹•
        combined_text = item['text']
        for j in range(1, 5):
            if i + j < len(transcript):
                combined_text += ' ' + transcript[i + j]['text']

        combined_normalized = normalize_text(combined_text)
        combined_words = set(combined_normalized.split())

        # è¨ˆç®—æ–‡å­—ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, sentence_normalized, combined_normalized).ratio()

        # è¨ˆç®—é—œéµè©åŒ¹é…ç‡ï¼ˆå¿½ç•¥å¸¸è¦‹è©ï¼‰
        common_words = {'i', 'a', 'the', 'is', 'it', 'to', 'and', 'in', 'my', 'you', 'we'}
        important_words = sentence_words - common_words
        if important_words:
            keyword_matches = len(important_words & combined_words)
            keyword_ratio = keyword_matches / len(important_words)
        else:
            keyword_ratio = 0.5  # å¦‚æœæ²’æœ‰é‡è¦å­—è©ï¼Œçµ¦ä¸­ç­‰åˆ†æ•¸

        # èˆ‡é æœŸæ™‚é–“çš„è·é›¢
        time_diff = abs(item['start'] - expected_start)

        # æ™‚é–“æ¬Šé‡ï¼šè¶Šæ¥è¿‘é æœŸæ™‚é–“åˆ†æ•¸è¶Šé«˜
        if time_diff <= 3:
            time_score = 1.0
        elif time_diff <= 10:
            time_score = 1.0 - (time_diff - 3) / 14  # 3-10ç§’ï¼š0.5-1.0
        elif time_diff <= 20:
            time_score = 0.5 - (time_diff - 10) / 40  # 10-20ç§’ï¼š0.25-0.5
        else:
            time_score = max(0, 0.25 - (time_diff - 20) / 80)

        # å°æ–¼çŸ­å¥ï¼Œæ™‚é–“æ¥è¿‘åº¦æ¬Šé‡æ›´é«˜ (50%)
        if is_short_sentence:
            score = similarity * 0.25 + keyword_ratio * 0.25 + time_score * 0.50
        else:
            # é•·å¥ï¼šæ–‡å­—ç›¸ä¼¼åº¦ 40% + é—œéµè© 30% + æ™‚é–“æ¥è¿‘åº¦ 30%
            score = similarity * 0.4 + keyword_ratio * 0.3 + time_score * 0.3

        # å¦‚æœå¥å­å®Œå…¨åŒ…å«åœ¨å­—å¹•ä¸­ä¸”æ™‚é–“éå¸¸æ¥è¿‘ï¼Œå¤§å¹…åŠ åˆ†
        if sentence_normalized in combined_normalized:
            if time_diff <= 5:
                score = max(score, 0.95)
            elif time_diff <= 10:
                score = max(score, 0.85)
            elif time_diff <= 20:
                score = max(score, 0.75)

        if score > 0.35:
            candidates.append({
                'start': item['start'],
                'text': combined_text[:100],
                'score': score,
                'time_diff': time_diff,
                'text_similarity': similarity,
                'keyword_ratio': keyword_ratio
            })

    if not candidates:
        return None

    # æŒ‰çµ„åˆåˆ†æ•¸æ’åºï¼Œç„¶å¾ŒæŒ‰æ™‚é–“å·®æ’åº
    candidates.sort(key=lambda x: (-x['score'], x['time_diff']))

    return candidates[0]


def verify_sentences(sentences: list[dict], use_cache: bool = True) -> dict:
    """é©—è­‰æ‰€æœ‰å¥å­çš„æ™‚é–“æˆ³"""
    results = {
        'passed': [],
        'warning': [],
        'failed': [],
        'errors': []
    }

    # æŒ‰å½±ç‰‡åˆ†çµ„
    videos = {}
    for s in sentences:
        vid = s['videoId']
        if vid not in videos:
            videos[vid] = []
        videos[vid].append(s)

    print(f"\nğŸ“‹ å…±æœ‰ {len(sentences)} å€‹å¥å­ï¼Œä¾†è‡ª {len(videos)} å€‹å½±ç‰‡\n")

    video_count = 0
    for video_id, video_sentences in videos.items():
        # é¿å… YouTube å°é–ï¼Œæ¯å€‹å½±ç‰‡ä¹‹é–“ç­‰å¾…
        if video_count > 0:
            print("   â³ ç­‰å¾… 3 ç§’é¿å…è¢«å°é–...")
            time.sleep(3)
        video_count += 1

        print(f"ğŸ¬ é©—è­‰å½±ç‰‡: {video_id}")
        print(f"   å¥å­æ•¸: {len(video_sentences)}")

        transcript = fetch_transcript(video_id, use_cache=use_cache)
        if not transcript:
            for s in video_sentences:
                results['errors'].append({
                    'id': s['id'],
                    'english': s['english'],
                    'error': 'ç„¡æ³•å–å¾—å­—å¹•'
                })
            continue

        for s in video_sentences:
            match = find_best_match(s['english'], transcript, s['startTime'])

            if not match:
                results['failed'].append({
                    'id': s['id'],
                    'english': s['english'],
                    'expected_start': s['startTime'],
                    'issue': 'åœ¨å­—å¹•ä¸­æ‰¾ä¸åˆ°åŒ¹é…'
                })
                print(f"   âŒ ID {s['id']}: æ‰¾ä¸åˆ°åŒ¹é… - \"{s['english'][:40]}...\"")
                continue

            time_diff = abs(match['start'] - s['startTime'])

            if time_diff <= 3:
                results['passed'].append({
                    'id': s['id'],
                    'english': s['english'],
                    'expected_start': s['startTime'],
                    'actual_start': match['start'],
                    'diff': time_diff,
                    'score': match['score']
                })
                print(f"   âœ… ID {s['id']}: OK (å·®ç•° {time_diff:.1f}s)")
            elif time_diff <= 10:
                results['warning'].append({
                    'id': s['id'],
                    'english': s['english'],
                    'expected_start': s['startTime'],
                    'actual_start': match['start'],
                    'diff': time_diff,
                    'score': match['score']
                })
                print(f"   âš ï¸  ID {s['id']}: æ™‚é–“å·® {time_diff:.1f}s - é æœŸ {s['startTime']}sï¼Œå¯¦éš› {match['start']:.1f}s")
            else:
                results['failed'].append({
                    'id': s['id'],
                    'english': s['english'],
                    'expected_start': s['startTime'],
                    'actual_start': match['start'],
                    'diff': time_diff,
                    'issue': f'æ™‚é–“å·®éå¤§ ({time_diff:.1f}s)'
                })
                print(f"   âŒ ID {s['id']}: æ™‚é–“å·®éå¤§ {time_diff:.1f}s - é æœŸ {s['startTime']}sï¼Œå¯¦éš› {match['start']:.1f}s")

        print()

    return results


def print_summary(results: dict):
    """å°å‡ºé©—è­‰æ‘˜è¦"""
    total = len(results['passed']) + len(results['warning']) + len(results['failed']) + len(results['errors'])

    print("=" * 60)
    print("ğŸ“Š é©—è­‰æ‘˜è¦")
    print("=" * 60)
    print(f"âœ… é€šé: {len(results['passed'])} / {total}")
    print(f"âš ï¸  è­¦å‘Š: {len(results['warning'])} / {total}")
    print(f"âŒ å¤±æ•—: {len(results['failed'])} / {total}")
    print(f"ğŸš« éŒ¯èª¤: {len(results['errors'])} / {total}")
    print()

    if results['warning']:
        print("âš ï¸  éœ€è¦æ³¨æ„çš„å¥å­:")
        for w in results['warning']:
            print(f"   ID {w['id']}: {w['english'][:50]}...")
            print(f"      é æœŸ {w['expected_start']}s â†’ å¯¦éš› {w['actual_start']:.1f}s (å·® {w['diff']:.1f}s)")
        print()

    if results['failed']:
        print("âŒ éœ€è¦ä¿®æ­£çš„å¥å­:")
        for f in results['failed']:
            print(f"   ID {f['id']}: {f['english'][:50]}...")
            if 'actual_start' in f:
                print(f"      é æœŸ {f['expected_start']}s â†’ å¯¦éš› {f['actual_start']:.1f}s")
            else:
                print(f"      å•é¡Œ: {f['issue']}")
        print()

    if results['errors']:
        print("ğŸš« ç™¼ç”ŸéŒ¯èª¤çš„å¥å­:")
        for e in results['errors']:
            print(f"   ID {e['id']}: {e['error']}")
        print()

    # è¨ˆç®—é€šéç‡
    if total > 0:
        pass_rate = (len(results['passed']) / total) * 100
        print(f"ğŸ“ˆ é€šéç‡: {pass_rate:.1f}%")

    # é¡¯ç¤ºå»ºè­°ä¿®æ­£ï¼ˆå¯è¤‡è£½è²¼ä¸Šï¼‰
    fixes_needed = [*results['warning'], *results['failed']]
    fixes_with_time = [f for f in fixes_needed if 'actual_start' in f]
    if fixes_with_time:
        print("\n" + "=" * 60)
        print("ğŸ“ å»ºè­°ä¿®æ­£ (åŸºæ–¼å­—å¹•æ™‚é–“æˆ³):")
        print("=" * 60)
        print("æ³¨æ„ï¼šä»¥ä¸‹å»ºè­°åŸºæ–¼ YouTube å­—å¹• API çš„æ™‚é–“æˆ³")
        print("å¯¦éš›æ’­æ”¾æ™‚é–“å¯èƒ½éœ€è¦æ‰‹å‹•å¾®èª¿ (é€šå¸¸æå‰ 1-2 ç§’)")
        print()
        for f in sorted(fixes_with_time, key=lambda x: x['id']):
            suggested_start = max(0, f['actual_start'] - 1)  # æå‰ 1 ç§’
            suggested_end = f['actual_start'] + 5  # å‡è¨­ 5 ç§’é•·åº¦
            print(f"ID {f['id']}: startTime: {suggested_start:.0f}, endTime: {suggested_end:.0f}")
            print(f"   å¥å­: {f['english'][:60]}")
            print()

    return len(results['failed']) == 0 and len(results['errors']) == 0


def main():
    import argparse

    parser = argparse.ArgumentParser(description='é©—è­‰ sentences.ts ä¸­çš„æ™‚é–“æˆ³')
    parser.add_argument('--no-cache', action='store_true', help='ä¸ä½¿ç”¨å¿«å–ï¼Œå¼·åˆ¶é‡æ–°å–å¾—å­—å¹•')
    parser.add_argument('--video', '-v', help='åªé©—è­‰æŒ‡å®šçš„å½±ç‰‡ ID')
    parser.add_argument('--ids', help='åªé©—è­‰æŒ‡å®šçš„å¥å­ ID (é€—è™Ÿåˆ†éš”ï¼Œå¦‚: 1,2,3)')
    args = parser.parse_args()

    print("ğŸ” YouTube æ™‚é–“æˆ³é©—è­‰å·¥å…·")
    print("=" * 60)

    # æ‰¾åˆ° sentences.ts æª”æ¡ˆ
    script_dir = Path(__file__).parent
    sentences_file = script_dir.parent / 'src' / 'data' / 'sentences.ts'

    if not sentences_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {sentences_file}")
        exit(1)

    print(f"ğŸ“ è®€å–: {sentences_file}")

    sentences = parse_sentences_ts(str(sentences_file))
    print(f"ğŸ“ è§£æåˆ° {len(sentences)} å€‹å¥å­")

    # éæ¿¾å¥å­
    if args.video:
        sentences = [s for s in sentences if s['videoId'] == args.video]
        print(f"ğŸ¯ éæ¿¾å½±ç‰‡ {args.video}: {len(sentences)} å€‹å¥å­")

    if args.ids:
        target_ids = set(int(x.strip()) for x in args.ids.split(','))
        sentences = [s for s in sentences if s['id'] in target_ids]
        print(f"ğŸ¯ éæ¿¾ ID {args.ids}: {len(sentences)} å€‹å¥å­")

    if not sentences:
        print("âŒ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„å¥å­")
        exit(1)

    results = verify_sentences(sentences, use_cache=not args.no_cache)
    success = print_summary(results)

    # å„²å­˜çµæœ
    results_file = script_dir / 'verification_results.json'
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ çµæœå·²å„²å­˜åˆ°: {results_file}")

    exit(0 if success else 1)


if __name__ == '__main__':
    main()
